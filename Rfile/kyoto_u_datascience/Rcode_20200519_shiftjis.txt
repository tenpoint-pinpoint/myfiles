# Rによる教育データ分析入門（ver. 2020/05/19）### ここに書かれたコードは、一部書籍と異なる場合があります。########## Chapter 1 ##########1 + 2# 引き算2 - 1# 掛け算2 * 3# 割り算4 / 2# 累乗3 ^ 4# 以下の処理の実行結果は全て同じ（実行結果は省略）1+21+ 21 +21 + 2    1     +     2# 変数に代入x <- 2# 変数の中身の確認x# 代入と同時に変数の中身を確認(x <- 2)# 変数を使った計算x + 1# 別の変数を作成y <- 3# 変数同士の計算x + y# ベクトルの作成と代入# c関数は，ベクトルを作成するための関数x <- c(1, 2, 3, 4, 5)# ベクトルの中身の確認x# ベクトルの長さ（要素数）の確認length(x)# ベクトルの3番目の要素だけを抽出x[3]# ベクトルの2番目から4番目の要素だけを抽出x[2 : 4]# ベクトルを使った計算x * 2# 別のベクトルを作成y <- c(6, 7, 8, 9, 10)# ベクトル同士の計算x + y# 行列の作成# ベクトルの用意z <- c(1, 2, 3, 4, 5, 6)# 行列の形式に変換matrix.1 <- matrix(z, nrow = 2, ncol = 3)matrix.1# matrix関数の引数byrowでTRUEを指定matrix.2 <- matrix(z, nrow = 2, ncol = 3, byrow = TRUE)matrix.2# 行数の確認nrow(matrix.2)# 列数の確認ncol(matrix.2)# 行列を使った計算matrix.2 + 1# 別の行列を作成（matrix関数とc関数を入れ子にする）matrix.3 <- matrix(c(7, 8, 9, 10, 11, 12), nrow = 2, ncol = 3, byrow = TRUE)# 行列同士の計算matrix.2 + matrix.3# 行列の結合（行方向）rbind(matrix.2, matrix.3)# 行列の結合（列方向）cbind(matrix.2, matrix.3)# 元の行列matrix.2# 2行目・3列目の要素を抽出matrix.2[2, 3]# 2行目の要素全てを抽出matrix.2[2, ]# 3列目の要素全てを抽出matrix.2[, 3]# 2行目の要素以外の全てを抽出matrix.2[-2, ]# 3列目の要素以外の全てを抽出matrix.2[, -3]# 元の行列matrix.2# 行列の転置t(matrix.2)matrix.2# 行ラベルの付与rownames(matrix.2) <- c("X", "Y")# 列ラベルの付与colnames(matrix.2) <- c("A", "B", "C")# ラベルの確認matrix.2# ベクトルの用意ID <- c("S001", "S002", "S003", "S004", "S005")Score <- c(75, 68, 82, 90, 78)# データフレームの作成df <- data.frame(ID, Score)# 作成したデータフレームの確認df# データの型（クラス）を確認class(df)class(df[, 1])class(df[, 2])# データフレームから一部の列のデータを抽出df$Score# 以下の処理と同じdf[, 2]# 作業ディレクトリの確認getwd()# 作業ディレクトリの変更# 以下は，「C」ドライブ直下の「Data」フォルダに変更する例setwd("C:/Data")# ファイルが作業ディレクトリにある場合dat <- read.csv("data_ch1-1.csv", header = FALSE)# ファイルが作業ディレクトリではなく，C:/Dataにある場合dat <- read.csv("C:/Data/data_ch1-1.csv", header = FALSE)dat <- read.csv(file.choose(), header = FALSE)dat# マウス操作でdata_ch1-1.csvを選択する場合dat <- read.csv(file.choose(), header = FALSE)# マウス操作でdata_ch1-2.csvを選択する場合dat.2 <- read.csv(file.choose(), header = TRUE)dat.2# マウス操作でdata_ch1-3.csvを選択する場合dat.3 <- read.csv(file.choose(), header = TRUE, row.names = 1)dat.3# ファイルへの書き出し（任意のファイル名を指定）write.table(dat.3, "output.csv", row.names = TRUE, col.names = NA, sep = ",")# パッケージのインストール（初回のみ）install.packages("foreign", dependencies = TRUE)# パッケージの読み込みlibrary("foreign")# c関数のヘルプを参照help(c)########## Chapter 2 ########### CSVファイルの読み込み（ヘッダーがある場合）# data_ch2.csvを選択dat <- read.csv(file.choose(), header = TRUE)# 読み込んだデータの冒頭の確認head(dat)# 行数と列数の確認dim(dat)# 平均値の計算（全員分の点数の総計を人数で割る）sum(dat$score) / nrow(dat)# 平均値の計算（mean関数）mean(dat$score)# 5名の平均点x <- c(70, 75, 80, 85, 90)mean(x)# 5名の平均点（外れ値がある場合）y <- c(0, 75, 80, 85, 90)mean(y)# 70，75，80，85，90の中央値median(x)# 0，75，80，85，90の中央値median(y)# 100名の学習者の期末試験結果の中央値median(dat$score)# 100名の学習者の期末試験結果の最頻値score.mode <- names(which.max(table(dat$score)))score.mode# 「文字列」から「数値」に変換as.numeric(score.mode)# 個々の点数をとった学習者の数を集計score.tab <- table(dat$score)score.tab# 100名の学習者の期末試験結果の最小値min(dat$score)# 100名の学習者の期末試験結果の最大値max(dat$score)# 最小値と最大値をまとめて計算range(dat$score)# データがばらついている範囲（最大値?最小値）max(dat$score) - min(dat$score)# 100名の学習者の期末試験結果の分散var(dat$score)# 100名の学習者の期末試験結果の標準偏差sd(dat$score)# 100名の学習者の期末試験結果の5値要約quantile(dat$score)# 100名の学習者の期末試験結果の要約統計量（5値要約，平均値）summary(dat$score)# パッケージのインストール（初回のみ）install.packages("psych", dependencies = TRUE)# パッケージの読み込みlibrary("psych")# 100名の学習者の期末試験結果の要約統計量（データ数，平均値，標準偏差，中央値，調整平均，中央絶対偏差値，最小値，最大値，範囲，歪度，尖度，標準誤差）describe(dat$score)# データの平均値を計算score.mean <- mean(dat$score)score.mean# データの標準偏差を計算score.sd <- sd(dat$score)score.sd# データの標準得点を計算(dat$score - score.mean) / score.sd# データの標準得点を計算（scale関数を使用）scale(dat$score)# データの偏差値を計算((dat$score - mean(dat$score)) / sd(dat$score)) * 10 + 50# データの偏差値を計算（scale関数を使用）scale(dat$score) * 10 + 50# 歪度skew(dat$score)# 尖度kurtosi(dat$score)########## Chapter 3 ########### CSVファイルの読み込み（ヘッダーがある場合）# data_ch3.csvを選択dat <- read.csv(file.choose(), header = TRUE)# 読み込んだデータの冒頭の確認head(dat)# 行数と列数の確認dim(dat)# クラス別の学習者数table(dat$class)# 担当教員別の学習者数table(dat$prof)# 男女別の学習者数table(dat$sex)# 学部別の学習者数table(dat$faculty)# scoreの記述統計量summary(dat$score)# 欠損値（NA）を含む行を削除dat.2 <- na.omit(dat)# 行数と列数の確認dim(dat.2)# scoreの記述統計量（欠損値を除外した場合）summary(dat.2$score)# データのコピーdat.3 <- dat# 欠損値を0に置換dat.3$score[is.na(dat.3$score)] <- 0# 行数と列数の確認dim(dat.3)# scoreの記述統計量（欠損値を0に置換した場合）summary(dat.3$score)# ヒストグラムの描画hist(dat.2$score)# ヒストグラムのタイトルと軸ラベルを変更hist(dat.2$score, main = "Final Exam", xlab = "score", ylab = "number of students")# ヒストグラムの色を変更hist(dat.2$score, main = "Final Exam", xlab = "score", ylab = "number of students", col = "white")# Rで使える色の確認colors()# パッケージのインストール（初回のみ）install.packages("lattice", dependencies = TRUE)# パッケージの読み込みlibrary("lattice")# 男女別の得点分布の比較histogram(~ score | sex, data = dat.2)# 学部別の得点分布の比較histogram(~ score | faculty, data = dat.2)# シンプルな箱ひげ図の描画boxplot(dat.2$score)# 箱ひげ図の作成に用いられている要約統計量の確認boxplot.stats(dat.2$score)# 箱ひげ図のタイトルと色を変更boxplot(dat.2$score, main = "Final Exam", col = "white")# クラス別の箱ひげ図boxplot(dat.2$score ~ dat.2$class)# 箱ひげ図の縦軸のスケールを変更boxplot(dat.2$score ~ dat.2$class, ylim = c(0, 100))# ノッチを入れた箱ひげ図boxplot(dat.2$score ~ dat.2$class, ylim = c(0, 100), notch = TRUE)# パッケージのインストール（初回のみ）install.packages("beeswarm", dependencies = TRUE)# パッケージの読み込みlibrary("beeswarm")# 蜂群図の描画beeswarm(dat.2$score ~ dat.2$class, ylim = c(0, 100), pch = 16, cex = 0.5)# 箱ひげ図に蜂群図を重ねて描画boxplot(dat.2$score ~ dat.2$class, ylim = c(0, 100))beeswarm(dat.2$score ~ dat.2$class, ylim = c(0, 100), pch = 16, cex = 0.5, add = TRUE)# パッケージのインストール（初回のみ）install.packages("gplots", dependencies = TRUE)# パッケージの読み込みlibrary("gplots")# 平均値±標準偏差のプロットplotmeans(dat.2$score ~ dat.2$class)# 各クラスの平均値tapply(dat.2$score, dat.2$class, mean)# 各クラスの標準偏差tapply(dat.2$score, dat.2$class, sd)# パッケージの読み込みlibrary("psych")# クラスごとの記述統計量describeBy(dat.2$score, dat.2$class)########## Chapter 4 ########### CSVファイルの読み込み（ヘッダーがある場合）# data_ch4_1.csvを選択dat <- read.csv(file.choose(), header = TRUE)# 読み込んだデータの冒頭の確認head(dat)# 行数と列数の確認dim(dat)# 読み込んだ4列のデータのうちNAのある行を削除dat.2 <- na.omit(dat)# NAのある行を削除したデータの冒頭を確認head(dat.2)# NAのある行を削除したデータの行数と列数の確認dim(dat.2)# クラス別の学習者数table(dat.2$class)# クラス別の男女の学習者数table(dat.2$class, dat.2$sex)# パッケージの読み込みlibrary("lattice")library("beeswarm")# クラス別の得点分布の比較（ヒストグラム）histogram(~ score | class, data = dat.2)# クラス別の得点分布の比較（箱ひげ図と蜂群図）boxplot(dat.2$score ~ dat.2$class, ylim = c(0, 100), main = "Result of the Exam", xlab = "class", ylab = "score")beeswarm(dat.2$score ~ dat.2$class, ylim = c(0, 100), pch = 16, add = TRUE)# パッケージの読み込みlibrary("psych")# クラスごとの記述統計量の計算describeBy(dat.2$score, dat.2$class)# 等分散性の検定# パッケージのインストール（初回のみ）install.packages("car", dependencies = TRUE)# パッケージの読み込みlibrary("car")leveneTest(dat.2$score, dat.2$class, center = mean)# 独立したt検定t.test(dat.2$score ~ dat.2$class)# CSVファイルの読み込み（ヘッダーがある場合）# data_ch4_2.csvを選択dat.3 <- read.csv(file.choose(), header = TRUE)# 読み込んだデータの冒頭の確認head(dat.3)# 行数と列数の確認dim(dat.3)# preとpostの列に欠損値がないか確認（TRUEがあれば，欠損値がある）table(is.na(dat.3[3 : 4]))# 男女の学習者数table(dat.3$sex)# 事前・事後テストの結果を重ねたヒストグラム（col = rgbで半透明の指定）hist(dat.3$pre, col = rgb(1, 0, 0, 0.5), xlim = c(30, 100), ylim = c(0, 15), main = "Overlapping Histogram", xlab = "score")hist(dat.3$post, col = rgb(0, 0, 1, 0.5), add = TRUE)# 箱ひげ図に個人の得点分布を重ねた蜂群図score <- c(dat.3$pre, dat.3$post)group <- factor(c(rep("pre", 30), rep("post", 30)), levels = c("pre","post"))boxplot(score ~ group, ylim = c(0, 100), main = "Result of the Pre-Post Test", xlab = "test", ylab = "score")beeswarm(score ~ group, ylim = c(0, 100), pch = 16, add = TRUE)# 記述統計量の確認describe(dat.3[, 3 : 4])# 対応のあるt検定t.test(dat.3$pre, dat.3$post, paired = TRUE)# p値の指数表示を回避options(scipen = 10)2.813e-05# 事前テストと事後テストの相関係数cor(dat.3$pre, dat.3$post)# パッケージのインストール（初回のみ）install.packages("latticeExtra", dependencies = TRUE)# パッケージの読み込みlibrary("lattice")library("latticeExtra")# 個別推移図（スパゲティ・プロット）df <- data.frame(score, group)df$indiv <- factor(c(rep(1 : nrow(dat.3)), rep(1 : nrow(dat.3))))each <- xyplot(score ~ group, group = indiv, type = c("l"), data = df, xlab = "test", ylab = "score")all_mean <- c(mean(dat.3$pre), mean(dat.3$post))fact <- factor(c("pre", "post"), levels = c("pre","post"))all <- xyplot(all_mean ~ fact, col = "black", lwd = 5, type = c("l"), data = df)each + as.layer(all, axes = NULL)# 横軸に事前テスト，縦軸に事後テストをプロットした散布図plot(dat.3$pre, dat.3$post, las = 1, pch = 16, xlab = "pretest", ylab = "posttest", main = NA, xlim = c(0, 100), ylim = c(0, 100))lines(par()$usr[1 : 2], par()$usr[3 : 4], lty = 3)########## Chapter 5 ########### CSVファイルの読み込み（ヘッダーがある場合）# data_ch5_1.csvを選択dat <- read.csv(file.choose(), header = TRUE)# 読み込んだデータの冒頭の確認head(dat)# 行数と列数の確認dim(dat)# クラス別の学習者数table(dat$class)# クラスごとの記述統計量の計算library("psych")describeBy(dat$score, dat$class)# 等分散性の検定library("car")leveneTest(dat$score, dat$class, center = mean)# パッケージの読み込みlibrary("beeswarm")# 蜂群図の描画boxplot(dat$score ~ dat$class, col = "grey", ylim = c(0, 100), main = "3クラスの比較", xlab = "class", ylab = "score")beeswarm(dat$score ~ dat$class, ylim = c(0, 100), pch = 16, add = TRUE)# 繰り返しのない一元配置分散分析# ANOVA君のソースを読み込んでから実行anovakun(dat[, -1], "As", 3, holm = TRUE, eta = TRUE)# CSVファイルの読み込み（ヘッダーがある場合）# data_ch5_2.csvを選択dat.2 <- read.csv(file.choose(), header = TRUE)# 読み込んだデータの冒頭の確認head(dat.2)# 行数と列数の確認dim(dat.2)# クラス別の学習者数table(dat.2$class)# 二元配置分散分析anovakun(dat.2[, -1], "AsB", 2, 3, auto = TRUE, holm = TRUE, eta = TRUE)# スタック形式に変更x <- stack(dat.2[, 3 : 5])# データフレームの作成y <- data.frame(dat.2$class, x)# 因子の型に変更y$dat.2.class <- factor(y$dat.2.class)# 水準の順序を指定y$ind <- factor(y$ind, levels = c("pre", "post", "delayed"))# データフレームの列名を変更names(y) <- c("class", "score", "test")# 交互作用確認プロットinteraction.plot(y$test, y$class, y$score, type = "b", pch = c(1, 2), xlab = "Test", ylab = "Score", trace.label = "Class")########## Chapter 6 ########### サンプルサイズ，平均値，標準偏差を指定してデータを作成する関数gendat <- function(n, mu = 0, sigma = 1)  {    x <- rnorm(n)    return((x - mean(x)) / sd(x) * sigma + mu)  }# データセットAの作成（サンプルサイズ，平均値，標準偏差の順）a <- gendat(50, 30.00, 10.00)b <- gendat(50, 32.00, 10.00)# データセットBの作成（サンプルサイズ，平均値，標準偏差の順）x <- gendat(500, 30.00, 10.00)y <- gendat(500, 32.00, 10.00)# t検定（データセットA）t.test(a, b)# t検定（データセットB）t.test(x, y)# 描画のためのデータを用意（因子型に変換）score.A <- c(a, b)group.A <- factor(c(rep("1 (n = 50)", length(a)), rep("2 (n = 50)", length(b)))) score.B <- c(x, y)group.B <- factor(c(rep("1 (n = 500)", length(x)), rep("2 (n = 500)", length(y))))# グラフを2つ並べて表示する設定par(mfrow = c(1, 2))# パッケージの読み込みlibrary("beeswarm")# 箱ひげ図と蜂群図の描画boxplot(score.A ~ group.A, ylim = c(0, 70), main = "データセットA", xlab = "指導法", ylab = "score")beeswarm(score.A ~ group.A, ylim = c(0, 70), pch = 16, cex = 0.5, add = TRUE)boxplot(score.B ~ group.B, ylim = c(0, 70), main = "データセットB", xlab = "指導法", ylab = "score")beeswarm(score.B ~ group.B, ylim = c(0, 70), pch = 16, cex = 0.5, add = TRUE)# パッケージのインストール（初回のみ）install.packages("compute.es", dependencies = TRUE)# パッケージの読み込みlibrary("compute.es")# データセットAの効果量算出mes(mean(a), mean(b), sd(a), sd(b), n.1 = 50, n.2 = 50)# データセットBの効果量算出mes(mean(x), mean(y), sd(x), sd(y), n.1 = 500, n.2 = 500)# 第4章の対応のないt検定のデータから効果量を計算mes(60.24, 72.27, 15.81, 16.11, 33, 37)# 第4章の対応のあるt検定のデータから効果量を計算# 対応なしの場合のdの計算res <- mes(67.33, 74.43, 9.66, 8.98, 30, 30)# 式（6.2）を使用# 事前テストと事後テストの相関係数は0.6487101# res[, "d"]は効果量dd.val <- res[, "d"] / sqrt(2 * (1 - 0.6487101))# 95%信頼区間の計算library("psych")d.ci(d.val, n1 = 30)# CSVファイルの読み込み（ヘッダーがある場合）# data_ch6-1.csvを選択dat <- read.csv(file.choose(), header = TRUE)# パッケージのインストール（初回のみ）install.packages("coin", dependencies = TRUE)# パッケージの読み込みlibrary("coin")# Mann?WhitneyのU検定res <- wilcox_test(dat$score ~ factor(dat$class), distribution = "exact")res# res@statistic@teststatisticは，上の結果のZ値と同じr <- abs(res@statistic@teststatistic) / sqrt(length(dat$score))r# 効果量rの95%信頼区間の算出library("psych")r.con(r, length(dat$score), p = .95, twotailed = TRUE)# CSVファイルの読み込み（ヘッダーがある場合）# data_ch6-2.csvを選択dat.2 <- read.csv(file.choose(), header = TRUE)# Wilcoxonの符号付順位和検定# パッケージのインストール（初回のみ）install.packages("exactRankTests", dependencies = TRUE)# パッケージの読み込みlibrary("exactRankTests")res.2 <- wilcox.exact(dat.2$pre, dat.2$post, paired = TRUE)res.2# 効果量rの計算z <- qnorm(1 - (res.2$p.value / 2))r <- z / sqrt(length(dat.2$pre) * 2)r# 効果量rの95%信頼区間の算出r.con(r, length(dat.2$pre * 2), p =.95, twotailed = TRUE)# CSVファイルの読み込み（ヘッダーがある場合）# data_ch6-3.csvを選択dat.3 <- read.csv(file.choose(), header = TRUE)# Kruskal-Wallis検定res.3 <- kruskal.test(dat.3$score ~ factor(dat.3$class))res.3# 効果量rの計算z.2 <- qnorm(1 - (res.3$p.value / 2))r.2 <- abs(z.2) / sqrt(nrow(dat.3))r.2# 効果量rの95%信頼区間の算出r.con(r.2, nrow(dat.3), p =.95, twotailed = TRUE)# 多重比較# 引数p.adjで"bonferroni"を指定すれば，Bonferroniの方法pairwise.wilcox.test(dat.3[, 2], dat.3[, 1], p.adj = "holm", exact = FALSE, correct = FALSE)# CSVファイルの読み込み（ヘッダーがある場合）# data_ch6-4.csvを選択dat.4 <- read.csv(file.choose(), header = TRUE)# Freedman検定（データを行列に変換）res.4 <- friedman.test(as.matrix(dat.4))res.4# 効果量rの計算z.3 <- qnorm(1 - (res.4$p.value / 2))r.3 <- abs(z.3) / sqrt(nrow(dat.4))r.3# 効果量rの95%信頼区間の算出r.con(r.3, nrow(dat.4), p =.95, twotailed = TRUE)# 型の変更dat.5 <- stack(dat.4) x <- dat.5[, 1]y <- dat.5[, 2]# 多重比較（引数pairedでTRUEを指定）pairwise.wilcox.test(x, y, p.adj = "holm", exact = FALSE, paired = TRUE, correct = FALSE)########## Chapter 7 ########### CSVファイルの読み込み# data_ch7-1.csvを選択dat <- read.csv(file.choose(), header = TRUE) # 1〜6列目のうちNAのある行を削除dat.2 <- na.omit(dat[, 1 : 6])# NAのある行を削除したデータの冒頭を確認head(dat.2)# midとend（5〜6列目）の記述統計summary(dat.2[, 5 : 6])# 中間試験と期末試験のPearsonの積率相関係数cor(dat.2$mid, dat.2$end, method = "pearson")# 相関係数の有意確率（無相関検定）や95%信頼区間の計算cor.test(dat.2$mid, dat.2$end, method = "pearson")# 対角線ありの散布図の作成plot(dat.2$mid, dat.2$end,      # ドットの大きさを変更     cex = 1.2,     # x軸・y軸のラベルと範囲を変更，タイトルを表示     xlab = "中間テスト得点", xlim = c(0, 100),     ylab = "期末テスト得点", ylim = c(0, 100))# 散布図に切片a = 0，傾きb = 1の直線を表示abline(a = 0, b = 1, lty = 1)# パッケージの読み込みlibrary("psych")# 中間試験（5列目）と期末試験（6列目）の散布図を作成pairs.panels(dat.2[, 5 : 6],      # 回帰直線（黒色）とその95%信頼区間を表示     lm = TRUE, ci = TRUE, col = "black",     # ヒストグラムの色を変更     hist.col = "grey",     # ドットのスタイルを変更     pch = 21,     # 有意な相関係数にアスタリスクを付ける     stars = TRUE,     # 平滑線（smooth）と相関円（ellipses）を描写しない     smooth = FALSE, ellipses = FALSE)# 学科F01だけで相関分析cor(dat.2$mid[dat.2$faculty == "F01"], dat.2$end[dat.2$faculty == "F01"], method = "pearson")# パッケージの読み込みlibrary("lattice")# 中間試験と期末試験の散布図を学科別にプロット（実行結果は省略）xyplot(mid ~ end | faculty, data = dat.2,       # 中間の点数が80点以上か否かで色分け，異なるマーカーを使用       groups = mid >= 80, col = c("grey20", "grey40"),       pch = c(1, 16), cex = c(1.0, 1.0),       # 中間試験と期末試験の平均点ラインを描写       abline = list(h = mean(dat.2$mid),v = mean(dat.2$end),       col = "grey"))# F01~F09の順番を並び替えることも可能dat.2$faculty <- factor(dat.2$faculty,        levels = c("F07", "F08", "F09", "F04", "F05", "F06",       "F01", "F02", "F03"))xyplot(mid ~ end | faculty, data = dat.2,         groups = mid >= 80, col = c("grey20", "grey40"),       pch = c(1, 16), cex = c(1.0, 1.0),       abline = list(h = mean(dat.2$mid),v = mean(dat.2$end),       col = "grey"))# 7〜9列目のうちNAのある行を削除dat.3 <- na.omit(dat[, 7 : 9])# Spearmanの相関係数を計算cor(dat.3, method = "spearman")# 3変数以上の組み合わせから相関係数の有意確率と95%信頼区間を計算res <- corr.test(dat.3, method = "spearman")print(res, short = FALSE)# 散布図plot(dat.3)# クロス集計表の作成ques <- xtabs( ~ manzoku + rikai, data = dat.3)ques# パッケージの読み込みlibrary("gplots")# 気球グラフのプロットballoonplot(ques)# CSVファイルの読み込み# data_ch7-2.csvを選択dat.4 <- read.csv(file.choose(), header = TRUE) # 読み込んだデータの冒頭の確認head(dat.4)# 1列目を除いたデータからCronbachのα係数を計算alpha(dat.4[, -1])# パッケージの読み込みlibrary("RMeCab") # 分析データの読み込み# data_ch7-3.csvを選択dat.5 <- read.csv(file.choose(), header = FALSE)# 形態素解析RMC <- RMeCabDF(dat.5)# 100名分の自由回答を結合RMC.2 <- unlist(RMC)# 結合したデータの冒頭を確認head(RMC.2)# 形態素解析の結果をデータフレームに変換RMC.3 <- data.frame(RMC.2, names(RMC.2))# 列名を編集colnames(RMC.3) <- c("Word", "POS")# データフレームの冒頭を確認head(RMC.3)# データフレームから名詞の行だけを抽出noun <- RMC.3[RMC.3$POS == "名詞", ]# 抽出した結果（冒頭の3つ）を確認head(noun, 3)# 名詞の頻度を集計noun.list <- table(noun[, 1])# 頻度の高い順に並び替えnoun.list.2 <- sort(noun.list, decreasing = TRUE)# 並び替え結果をデータフレームに変換noun.list.3 <- data.frame(noun.list.2)# 列名を編集colnames(noun.list.3) <- c("Word", "Freq")# 頻度上位の名詞（冒頭の3つ）を確認head(noun.list.3, 3)# KWICコンコーダンスを作成する関数の定義 kwic.conc <- function(vector, word, span){  word.vector <- vector  # 分析対象とするベクトルを指定  word.positions <- which(word.vector == word)  # 分析対象とする単語の出現位置を特定  context <- span  # 単語の文脈を左右何語ずつ表示するかを指定  # 用例を検索  for(i in seq(word.positions)) {       if(word.positions[i] == 1) {         before <- NULL      } else {      start <- word.positions[i] - context      start <- max(start, 1)      before <- word.vector[start : (word.positions[i] - 1)]  }  end <- word.positions[i] + context  after <- word.vector[(word.positions[i] + 1) : end]  after[is.na(after)] <- ""  keyword <- word.vector[word.positions[i]]  # 用例を表示  cat("--------------------", i, "--------------------", "\n")  cat(before, "[", keyword, "]", after, "\n")  }}# 「教室」の用例検索kwic.conc(RMC.2, "教室", 5)########## Chapter 8 ########### CSVファイルの読み込み# data_ch8-1.csvを選択dat <- read.csv(file.choose(), header = TRUE, row.names = 1)# 欠損値（NA）の有無を確認subset(dat, complete.cases(dat) == FALSE)# 中間（1列目）と期末（2列目）の成績を抽出し，NAを含む行を削除dat.2 <- na.omit(dat[, 1 : 2])# 中間と期末の成績を抽出し，NAを含む行を削除したデータの冒頭を確認head(dat.2)# Mahalanobisの距離d <- mahalanobis(dat.2, apply(dat.2, 2, mean), cov(dat.2))# データの行数と列数の計算n <- nrow(dat.2)v <- ncol(dat.2)# 外れ値の検出out <- n * (n - v) / ((n ^ 2 - 1) * v) * d > qf(0.9, n, v)# 散布図に外れ値を表示plot(dat.2, pch = ifelse(out, 16, 21), xlim = c(0, 100), ylim = c(0, 100), xlab = "中間試験", ylab = "期末試験")# 外れ値の行番号とラベルを表示text(dat.2[out, ] - 3, labels = paste(which(out), ":", rownames(dat.2)[out]))# 外れ値の除外dat.3 <- dat.2[-which(out == TRUE), ]# 単回帰モデル（結果変数 ~ 予測変数）model.1 <- lm(end ~ mid, data = dat.3)summary(model.1)# 回帰直線の可視化plot(dat.3, xlim = c(0, 100), ylim = c(0, 100), xlab = "中間試験", ylab = "期末試験")abline(model.1)# 傾きと切片の95%信頼区間confint(model.1, level = 0.95)# 新しいデータ（0点から100点まで1点刻み）を用意new <- data.frame(mid = seq(0, 100, 1))# 95%信頼区間の可視化confidence <- predict(model.1, newdata = new, interval = 'confidence', level = 0.95)lines(new$mid, confidence[, 2], lty = 3)lines(new$mid, confidence[, 3], lty = 3)# 中間試験が60点の場合の期末試験の点数の予測pred <- predict(model.1, newdata = new, interval = 'prediction', level = 0.95)pred[60, ]# 予測区間の可視化plot(dat.3, xlim = c(0, 100), ylim = c(0, 100), xlab = "中間試験", ylab = "期末試験")abline(model.1)lines(new$mid, pred[, 2], lty = 2)lines(new$mid, pred[, 3], lty = 2)# 残差の等分散性と正規性のプロットpar(mfcol = c(1, 2))plot(model.1, which = c(1, 2))# 分析データの準備dat.4 <- na.omit(dat)# パッケージのインストール（初回のみ）install.packages("pequod", dependencies = TRUE)# パッケージの読み込みlibrary("pequod")# 外れ値の除外d.2 <- mahalanobis(dat.4, apply(dat.4, 2, mean), cov(dat.4))n.2 <- nrow(dat.4)v.2 <- ncol(dat.4)out.2 <- n.2 * (n.2 - v.2) / ((n.2 ^ 2 - 1) * v.2) * d.2 > qf(0.9, n.2, v.2)dat.4[out.2, ]dat.5 <- dat.4[-which(out.2 == TRUE), ]# 重回帰モデルmodel.2 <- lmres(end ~ mid + quiz, data = dat.5)summary(model.2)# 回帰平面の作成install.packages("scatterplot3d", dependencies = TRUE)library(scatterplot3d)plot3d <- scatterplot3d(dat.4$mid, dat.4$quiz, dat.4$end, type = "n",   xlim = c(0, 100), ylim = c(0, 50), zlim = c(0, 100), angle = 25,  xlab = "?中間試験", ylab = "?期末試験", zlab = "?小テスト")plot3d$plane3d(lm(end ~ mid + quiz, data = dat.4),  lty.box = "solid", col = "grey", draw_polygon = TRUE, draw_lines = TRUE,   polygon_args = list(col = rgb(0.97, 0.97, 0.97, 0.97)))orig <- plot3d$xyz.convert(dat.4$mid, dat.4$quiz, dat.4$end)plane <- plot3d$xyz.convert(dat.4$mid, dat.4$quiz, fitted(lm(end ~ mid + quiz, data = dat.4)))i.negpos <- 1 + (resid(lm(end ~ mid + quiz, data = dat.4)) > 0)segments(orig$x, orig$y, plane$x, plane$y,  lty = c(2, 1)[i.negpos])wh1 <- resid(lm(end ~ mid + quiz, data = dat.4)) > 0wh2 <- resid(lm(end ~ mid + quiz, data = dat.4)) < 0segments(orig$x[wh1], orig$y[wh1], plane$x[wh1], plane$y[wh1], lty = 1)plot3d$points3d(dat.4$mid[wh1], dat.4$quiz[wh1], dat.4$end[wh1], pch = 19)plot3d$points3d(dat.4$mid[wh2], dat.4$quiz[wh2], dat.4$end[wh2], pch = 21)# 交互作用の検討model.3 <- lmres(end ~ mid + quiz + mid : quiz, centered = c("mid", "quiz"), data = dat.5)summary(model.3)# 単純傾斜の検定eff.1 <- simpleSlope(model.3, pred = "mid", mod1 = "quiz")summary(eff.1)eff.2 <- simpleSlope(model.3, pred = "quiz", mod1 = "mid")summary(eff.2)# 単純傾斜のプロットPlotSlope(eff.1)PlotSlope(eff.2)# 検定によるモデル選択anova(model.1, model.2$Stepfin, model.3$Stepfin)# 情報量基準によるモデル選択AIC(model.1, model.2$Stepfin, model.3$Stepfin)# 分析データの読み込み# data_ch8-2.csvを選択dat <- read.csv(file.choose(), row.names = 1, header = TRUE)# 読み込んだデータの冒頭を確認head(dat)# 散布図の作成と回帰直線の描画plot(dat$Hour, dat$TOEIC)linear.model <- lm(TOEIC ~ Hour, data = dat)abline(linear.model)# 個々の学習者の情報を表示した散布図plot(dat$Hour, dat$TOEIC, pch = dat$School)# 学校ごとに回帰分析linear.model.2 <- lm(TOEIC[1 : 40] ~ Hour[1 : 40], data = dat)linear.model.3 <- lm(TOEIC[41 : 80] ~ Hour[41 : 80], data = dat)linear.model.4 <- lm(TOEIC[81 : 120] ~ Hour[81 : 120], data = dat)# 回帰直線の表示abline(linear.model.2, lty = 1)abline(linear.model.3, lty = 2)abline(linear.model.4, lty = 3)# パッケージのインストール（初回のみ）install.packages("ICC", dependencies = TRUE)# パッケージの読み込みlibrary("ICC")# 級内相関ICCest(TOEIC, Hour, data = dat)# パッケージのインストール（初回のみ）install.packages("lme4", dependencies = TRUE)# パッケージの読み込みlibrary("lme4")# マルチレベル分析multilevel.model <- lmer(TOEIC ~ Hour + (Hour | School), data = dat)# マルチレベル分析の結果を確認summary(multilevel.model)# マルチレベル分析から得られた係数fixef(multilevel.model)# 通常の回帰分析から得られた係数linear.model$coefficients# 散布図上で比較plot(dat$Hour, dat$TOEIC, pch = dat$School)abline(fixef(multilevel.model), lty = 1)abline(linear.model$coefficients, lty = 2)# 勉強時間の影響を除いた学校間の点数比較multilevel.model.2 <- lmer(TOEIC ~ Hour + (1 | School), data = dat)summary(multilevel.model.2)########## Chapter 9 ########### CSVファイルの読み込み# data_ch9-1.csvを選択dat <- read.csv(file.choose(), header = TRUE)# psychパッケージの読み込みlibrary("psych")# 記述統計の確認 (1列目のstudentの列は分析に含めない)describe(dat[, -1])# KMOによるサンプリングの適切性指標を確認KMO(dat[, -1])# Bartlettの球面性検定cortest.bartlett(cor(dat[, -1]), n = nrow(dat))# 観測変数間の相関係数を確認するpairs.panels(dat[, -1], lm = TRUE, density = FALSE)# 因子数の決定（1）# 固有値の計算r.eigen <- eigen(cor(dat[, -1]))print(r.eigen$values, digit = 2)# スクリープロットの作成plot(r.eigen$values, xlim = c(1, 9),  xaxp = c(1, 9, 8), type = "b", xlab = "因子の番号", ylab = "固有値")abline(h = 1)# 因子数の決定（2）# MAP基準の確認VSS(dat[, -1], n = nrow(dat), fm = "ml")# 因子数の決定（3）# 平行分析fa.parallel(cor(dat[, -1]), fm = "ml", n.obs = nrow(dat), n.iter = 100)# 最尤法・オブリミン回転による因子分析r.fa <- fa(dat[, -1], nfactors = 3, fm = "ml", rotate = "oblimin")print(r.fa, sort = TRUE, digits = 2)# lavaanパッケージの読み込みlibrary("lavaan")# 3因子構造のモデルを記述（''を忘れやすいので注意）model.1 <- '  LV.1 =~ item8 + item9 + item4  LV.2 =~ item3 + item7  LV.3 =~ item2 + item1 + item6 '# 確認的因子分析fit.1 <- cfa(model.1, data = dat, estimator = "ML")summary(fit.1, fit.measures = TRUE, standardized = TRUE)# 適合度指標の検討fitMeasures(fit.1)# 2因子構造のモデルを記述model.2 <- '  LV.1 =~ item8 + item9 + item4  LV.2 =~ item2 + item1 + item6 '# 確認的因子分析fit.2 <- cfa(model.2, data = dat, estimator = "ML")summary(fit.2, fit.measures = TRUE, standardized = TRUE)# model1とmodel2の比較anova(fit.1, fit.2)# 因子得点の計算lavPredict(fit.2)# 尺度得点の計算LV.1 <- rowMeans(dat[, c(9, 10, 5)])LV.2 <- rowMeans(dat[, c(3, 2, 7)])cbind(LV.1, LV.2)# CSVファイルの読み込み（ヘッダーがある場合）# data_ch9-2.csvを選択dat.A <- read.csv(file.choose(), header = TRUE)# data_ch9-3.csvを選択dat.B <- read.csv(file.choose(), header = TRUE)# パッケージのインストール（初回のみ）install.packages("ltm", dependencies = TRUE)# パッケージの読み込みlibrary("ltm") # 1パラメター（Rasch）モデルで分析test.A <- rasch(dat.A[, -1])test.B <- rasch(dat.B[, -1])# 2つの図を1つにまとめて表示する設定par(mfrow = c(1, 2))# 項目特性曲線の作成plot(test.A, type = "ICC", items = c(23, 24, 29))# 項目情報量曲線の作成plot(test.A, type = "IIC", items = c(23, 24, 29))# 能力推定値の計算theta.A <- factor.scores.rasch(test.A, resp.pattern = dat.A[, -1])theta.B <- factor.scores.rasch(test.B, resp.pattern = dat.B[, -1])A.theta <- theta.A$score.dat$z1B.theta <- theta.B$score.dat$z1# 計算結果の冒頭を確認head(A.theta)head(B.theta)# 図9.6raw　<-　rowSums(dat.A[, -1])cbind(raw, A.theta)par(ps = 20, mai = c(1, 1, 1, 1), mfrow = c(1, 2))hist(A.theta, main = "", xlab = "能力推定値", ylab = "度数")abline(v = mean(A.theta), lty = 2)plot(raw, A.theta, pch = 1, xlab = "素点", ylab = "能力推定値", cex = 1.5)abline(lm(A.theta ~ raw), col = "black", lty = 2)# 共通項目の難易度の平均値(dffcltA.mean <- mean(test.A$coefficients[1:20, 1]))(dffcltB.mean <- mean(test.B$coefficients[1:20, 1]))# 変換式の作成Intercept <- dffcltA.mean - dffcltB.mean# テストB受験者の能力推定値をテストAに合わせる処理B.adjusted <- B.theta + Intercept# 等化前と等化後のテストＢ受験者の能力推定値の変化を箱ひげ図で可視化boxplot(A.theta, B.theta, B.adjusted, names = c("テストA", "テストB（等化前）", "テストB（等化後）"), main = NA, xlab = NA, ylab = "能力推定値", col = "grey")# CSVファイルの読み込み# data_ch9-3.csvを選択dat.C <- read.csv(file.choose(), header = TRUE)# パッケージのインストール（初回のみ）install.packages("mirt", dependencies = TRUE)# パッケージの読み込みlibrary("mirt")# 段階反応モデルで分析test.C <- mirt(data = dat.C[, -1], model = 1, itemtype = "graded")# 能力推定値の計算C.theta <- fscores(test.C)C.theta# 項目特性曲線の作成plot(test.C, type = 'trace', which.items = c(6, 10))########## Chapter 10 ########### パッケージのインストール（初回のみ）install.packages("semPlot", dependencies = TRUE)# パッケージの読み込みlibrary("lavaan")library("semPlot")# CSVファイルの読み込み（data_ch10-1.csvを選択）dat <- read.csv(file.choose(),fileEncoding = "shift-jis", header = TRUE)# パス解析（逐次モデルの指定）model.RM <- '  興味関心 ~ 学生対応 + 説明  理解度 ~ 説明 + 興味関心 '# 分析と結果の表示fit.RM <- sem(model.RM, data = dat, estimator = "ML")summary(fit.RM, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)# パス図の作成semPaths(fit.RM, what = "stand", style = "lisrel", layout = "tree", rotation = 2, nCharNodes = 0, nCharEdges = 0, fade = FALSE, edge.width = 0.2, label.scale = FALSE, label.cex = 1.2, theme = 'gray', asize = 6.0, node.width = 2.0, curve = 2.0)# CSVファイルの読み込み（data_ch10-2.csvを選択）dat.2 <- read.csv(file.choose(),fileEncoding = "shift-jis", header = TRUE)# MIMICモデルの指定model.MIMIC <- '  指導技術 =~ 進み具合 + 興味関心 + 理解度  指導技術 ~ 学生対応 + 話し方 + 説明 '# 分析と結果の表示fit.MIMIC <- sem(model.MIMIC, data = dat.2, estimator = "ML")summary(fit.MIMIC, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)# パス図の作成semPaths(fit.MIMIC, what = "stand", style = "lisrel", layout = "tree", rotation = 2, nCharNodes = 0, nCharEdges = 0, fade = FALSE, optimizeLatRes = TRUE, edge.width = 0.2, label.scale = FALSE, label.cex = 1.2, theme = 'gray', asize = 6.0, node.width = 1.5)# CSVファイルの読み込み（data_ch10-3.csvを選択）dat.3 <- read.csv(file.choose(), fileEncoding = "shift-jis", header = TRUE)# 多重指標モデルの指定model.MIC <- '  授業満足度 =~ item8 + item9 + item4  指導技術 =~ item2 + item1 + item6  授業満足度 ~ 指導技術 '# 分析と結果の表示fit.MIC <- sem(model.MIC, data = dat.3, estimator = "ML")summary(fit.MIC, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)# パス図の作成semPaths(fit.MIC, what = "stand", style = "lisrel", layout = "tree", rotation = 2, nCharNodes = 0, nCharEdges = 0,  fade = FALSE, optimizeLatRes = TRUE, edge.width = 0.2, label.scale = FALSE, label.cex = 1.2, theme = 'gray', asize = 6.0, node.width = 1.5)# CSVファイルの読み込み（data_ch10-4.csvを選択）dat.4 <- read.csv(file.choose(),fileEncoding = "shift-jis", header = TRUE)# 交差遅延モデルmodel.CLM <- '  単語力2 ~ 単語力1 + 多読量1  多読量2 ~ 単語力1 + 多読量1 '# 分析と結果の表示fit.CLM <- sem(model.CLM, data = dat.4, estimator = "ML")summary(fit.CLM, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)# パス図の作成semPaths(fit.CLM, what = "stand", style = "lisrel", layout = "tree2", rotation = 2, nCharNodes = 0, nCharEdges = 0, fade = FALSE, edge.width = 0.2, edge.label.position = 0.7, label.scale = FALSE, label.cex = 1.2, theme = 'gray', asize = 6.0,  node.width = 2.0)# CSVファイルの読み込み（data_ch10-5.csvを選択）dat.5 <- read.csv(file.choose(), fileEncoding = "shift-jis", header = TRUE)# 潜在成長曲線モデルの指定model.LGM <- '  切片 =~ 1 * 一学期 + 1 * 二学期 + 1 * 三学期   傾き =~ 0 * 一学期 + 1 * 二学期 + 2 * 三学期 '# 分析と結果の表示fit.LGM <- growth(model.LGM, data = dat.5, estimator = "ML")summary(fit.LGM, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)# パス図の作成semPaths(fit.LGM, title = FALSE, whatLabels = "est", style = "lisrel", layout = "tree", rotation = 1, nCharNodes = 0, nCharEdges = 0, fade = FALSE, optimizeLatRes = TRUE, edge.width = 0.2, edge.color = "black", label.scale = FALSE, label.cex = 1.2, theme = 'gray', asize = 6.0, node.width = 1.0)# モデルの修正指標modificationindices(fit.RM)# CSVファイルの読み込み（ヘッダーがある場合）# data_ch10-6.csvを選択dat <- read.csv(file.choose(), header = TRUE)# 未受験者（NA）を削除dat <- na.omit(dat)# 素点を偏差値に換算dat <- scale(dat[, -1]) * 10 + 50# 読み込んだデータの冒頭の確認head(dat)# 関数の定義（コードは自動で読み込まれます）source("C:/.../filename.txt")# 最適なランクを探索for(i in 2 : 6){  print(i)  r.LRT <- LRA(dat, i)  AIC(r.LRT)}# 分析結果の確認r.LRT <- LRA(dat, 5)summary(r.LRT)# ランクメンバーシッププロファイルの確認r.LRT$rank# ランクメンバーシッププロファイルの可視化par(mfrow = c(1, 2))plot(r.LRT$res[1, ], type = "b", xlab = "Rank")plot(r.LRT$res[9, ], type = "b", xlab = "Rank")########## Chapter 11 ########### CSVファイルの読み込み（ヘッダーと列名がある場合）# data_ch11-1.csvを選択dat <- read.csv(file.choose(), header = TRUE, row.names = 1)# 読み込んだデータの冒頭の確認head(dat)# 行数と列数の確認dim(dat)# 各科目の平均点（＝列平均）を計算colMeans(dat)# 各学習者の平均点（＝行平均）を計算rowMeans(dat)# ユークリッド距離の計算dat.d <- dist(dat)# ユークリッド距離の計算結果の確認dat.d# ウォード法によるクラスターの作成dat.hc <- hclust(dat.d, method = "ward.D2")dat.hc# 樹形図による可視化plot(dat.hc)# 樹形図における個々のデータを一番下にそろえて配置plot(dat.hc, hang = -1)# 個々のデータが含まれるクラスターの確認（クラスター数が2の場合）cutree(dat.hc, k = 2)# 樹形図による可視化plot(dat.hc, hang = -1)# 樹形図におけるクラスターを囲む四角形を表示（クラスター数が2の場合）rect.hclust(dat.hc, k = 2)# 分析データの形式を確認class(dat)# 分析データを行列に変換dat.2 <- as.matrix(dat)# 分析データの形式を再確認class(dat.2)# ヒートマップつきの樹形図の作成（ユークリッド距離，ウォード法）heatmap(dat.2, cexCol = 0.5, hclustfun = function(x){hclust(x, method = "ward.D2")})# 乱数の種を固定set.seed(1)# k-means法（クラスター数は2）dat.km <- kmeans(dat, center = 2)# k-means法によるクラスタリング結果の確認dat.km$cluster# パッケージの読み込みlibrary("cluster")# k-means法によるクラスタリング結果の可視化clusplot(dat, dat.km$cluster)# k-means法によるクラスタリング結果の可視化（個々の学習者のIDを表示）clusplot(dat, dat.km$cluster, labels = 2)# Gap統計量の計算set.seed(1)dat.Gap <- clusGap(dat, kmeans, K.max = 5)# 計算されたGap統計量の確認dat.Gap# Gap統計量とクラスター数の関係の可視化plot(dat.Gap)# CSVファイルの読み込み（ヘッダーがある場合）# data_ch11-2.csvを選択dat <- read.csv(file.choose(), header = TRUE)# 読み込んだデータの冒頭の確認head(dat)# 読み込んだデータ（全体）の概要の確認summary(dat)# テストの合格者数と不合格者数を確認table(dat$Grade)# パッケージの読み込みlibrary("rpart")# 決定木分析（Gradeを結果変数，それ以外を予測変数として指定）rpart.result <- rpart(Grade ~ ., data = dat)# 分析結果の確認rpart.result# パッケージのインストール（初回のみ）install.packages("partykit", dependencies = TRUE)# パッケージの読み込みlibrary("partykit")# 決定木の可視化plot(as.party(rpart.result))