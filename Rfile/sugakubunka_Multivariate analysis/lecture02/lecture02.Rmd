---
title: "第2回 : 線形回帰のモデル選択・回帰診断・外挿"
output: html_notebook
---

# 1. 前準備
今回は、線形回帰にまつわる話題から

* モデル選択（変数選択）
* 残差分析
* 外挿

を解説していきます。デモデータには前回同様 `salary.csv`、ある会社の社員に関する `月給`、`勤続年数`、`仕事の達成度`、`欠勤日数`、`特殊免許の有無` の情報が記録されたデータを用います。

社員の `月給` の傾向を `勤続年数`、`仕事の達成度`、`欠勤日数`、`特殊免許の有無` の4変数で説明する線形回帰は

$$
\begin{align*}
月給 &= \beta_0+\beta_1\times勤続年数 + \beta_2\times仕事の達成度 + \beta_3\times欠勤日数 + \beta_4\times特殊免許の有無 + 誤差
\end{align*}
$$

と表すことができ、偏回帰係数 $\beta_0,\beta_1,\cdots,\beta_4$ の推定値はR言語の `lm` 関数を用いて次のように推定することができるのでした。（復習されたい方は、第1回の資料をご覧ください。）

```{r}
# 線形回帰の計算結果
dat <- read.csv(file = "./data/salary.csv", 
                fileEncoding = "utf-8")
result <- lm(formula = 月給 ~ ., data = dat)
summary(result)
```


# 2. 線形回帰のモデル選択
変数 $x_{1},\cdots,x_{D}$ を用いて変数 $y$ を説明するような線形回帰

$$
\begin{align*}
y &= \beta_0+\beta_1x_1+\cdots+\beta_Dx_D+誤差
\end{align*}
$$
を作るとき、説明変数 $x_1,\cdots,x_D$ の組み合わせは事前知識によって入念に検討されて決まることが一般的です。しかし実際には、変数 $x_{d}$ を説明変数に含めた方が良いかいなか悩ましい変数もあるでしょう。そこで、何らかの基準に基づいて変数を選択する方法がいくつか提案されてきました。

* 偏回帰係数の統計的仮説検定
* 自由度調整済み決定係数
* 赤池情報量規準（AIC）

これらの手法を、以下の疑問に答える形で解説します。

**Remark** : 他にもMallows' Cp規準やベイズ情報量規準、正則化を用いる方法などがよく知られています。

**<font color = 59B9C6>問題</font>** : 変数 `欠勤日数` を説明変数に含めるかいなか、上記の3通りの方法で検討してみてください。

## A. 偏回帰係数の統計的仮説検定
`欠勤日数` を説明変数に含めるかいなかを検討する方法の一つに、`欠勤日数` の偏回帰係数が $0$ かいなかを検討するという考え方があります。これは、偏回帰係数の統計的仮設検定によって実現できます。

**<font color = 59B9C6>解答</font>** : 前回紹介した偏回帰係数の統計的仮説検定を、モデル選択に用いてみましょう。ここで考える帰無仮説と対立仮説は、

$$
\begin{align*}
H_0 : \beta_3 = 0 \text{ v.s. } H_1 : \beta_3 \neq 0
\end{align*}
$$

です。このとき、検定統計量の値（t値）は

\begin{align*}
t &= \frac{263-0}{976}\sim 0.269
\end{align*}

です。

```{r}
# Rでt値を計算してみよう。
t_value <- 263 / 976
t_value
```

両側検定であることに注意すると、有意水準 $5\%$ のとき棄却域は$|t|\geq 1.99$ （R言語を用いて `qt(p=0.975, df=100-(4+1)) `と計算できます）です。

```{r}
# Rで両側検定の棄却限界を計算してみよう。
critical <- qt(p = 0.975, df = 100-(4+1))
critical
```

すなわち、帰無仮説は棄却されません。すなわち「欠勤日数は説明変数として含めなくて良いだろう」という結果になります。なおこの結果は、`summary` 関数の出力 `t-value`, `Pr(>|t|)` によって確認することもできます。


## B. 自由度調整済み決定係数
`欠勤日数` を説明変数に含めるかいなかを検討するもう一つの方法に、目的変数を精度よく予測する式が作れるかいなかという考え方があります。偏回帰係数を推定して得られる式の予測の精度を測る指標に、自由度調整済み決定係数があります。

**自由度調整済み決定係数**（adjusted R-squared）は、目的変数に対する予測の精度を、線形回帰によるものと標本平均によるものとで比較したものです。$\hat{y_i}$ を標本点 $i$ の目的変数の値 $y_i$ に対する線形回帰の予測値、$\bar{y}$ を目的変数の標本平均とします。このとき、自由度調整済み決定係数は次のように定義されます。

$$
\begin{align*}
R^{*2} &= 1-\frac{\displaystyle\sum_{i=1}^{n}(\hat{y_{i}}-y_{i})^2/\{n-(D+1)\}}{\displaystyle\sum_{i=1}^{n}(\bar{y}-y_{i})^2/(n-1)}
\end{align*}
$$
自由度調整済み決定係数は、$1$ 以下の値をとります。なお、負の数になることもあります。この値が $1$ に近い線形回帰のほうが予測の精度が高いと推定できます。

**Remark** : 自由度調整済み決定係数の分子は線形回帰モデルが正しいと仮定したときの誤差の分散の不偏推定量、分母はなんら説明変数を用いなかったときの誤差の分散の不偏推定量になっています。このため、自由度調整済み決定係数は決定係数に比べて、母集団の決定係数を偏りなく推定することができます。つまり、決定係数は偏回帰係数の推定に用いたデータへのあてはまりの良さ、自由度調整済み決定係数は未知のデータへのあてはまりの良さの指標だと解釈できます。

**<font color = 59B9C6>解答</font>** : 
自由度調整済み決定係数は、`summary` 関数の `Adjusted R-squared` に書かれています。今回、変数 `欠勤日数` を含む線形回帰の自由度調整済み決定係数は `0.7651` です。一方、`欠勤日数` を含まない自由度調整済み決定係数は、以下のスクリプトから `0.7674` とわかります。

```{r}
result2 <- lm(formula = 月給 ~ 勤務年数+仕事の達成度+特殊免許の有無, 
              data = dat)
summary(result2)
```

変数 `欠勤日数` を含む線形回帰より、含まない線形回帰のほうが自由度調整済み決定係数が高く、`欠勤日数` を含まない場合の線形回帰のほうが未知のデータに対する予測の精度が高いと推定されます。すなわち「欠勤日数は説明変数として含めなくて良いだろう」という結果になります。


## C. 赤池情報量規準
未知のデータへの予測の精度を推定する方法には、自由度調整済み決定係数の他にも**赤池情報量規準**（Akaike Information Criterion, AIC）を使う方法があります。赤池情報量規準は、次のように定義されます。

$$
\begin{align*}
AIC &= n\log\sum_{i=1}^{n}(\hat{y_i}-y_i)^2+2(D+1)
\end{align*}
$$

この値が小さい線形回帰のほうが予測の精度が高いと推定できます。

**Remark** : 赤池情報量規準は、Kullback-Leibler divergenceとよばれる母集団と推定によって得た線形回帰の間の近さを測る指標の（漸近）不偏推定量になっています。

**<font color = 59B9C6>解答</font>** : 赤池情報量規準は、`AIC` 関数を用いて計算することができます。

```{r}
# 欠勤日数を含む場合のAICと、含まない場合のAIC
AIC(result); AIC(result2)
```

この結果、欠勤日数を含む場合の赤池情報量規準の値として $2144.875$、含まない場合の赤池情報量規準の値として $2142.952$ が得られ、`欠勤日数` を含まない場合の線形回帰のほうが未知のデータに対する予測の精度が高いと推定されます。すなわち「欠勤日数は説明変数として含めなくて良いだろう」という結果になります。



# 3. 回帰診断
線形回帰モデルの偏回帰係数を最小2乗法で推定することは、以下の場合に「適切な」推定になっていることが数理統計学で確認されてきました。

* 誤差 $\epsilon$ の分散が説明変数によらず一定な場合。
* 誤差が正規分布に従っていると仮定する。

深くは立ち入りませんが、前者の場合はGauss-Markovの定理とよばれるもの、後者の場合は最尤法と最小2乗法が等しくなるというものです。（詳しくは数理統計学の講座を受講されてみてください。）

また、最小2乗法は残差の「二乗和」を小さくするように偏回帰係数を求めるため、**外れ値**に敏感であることが知られています。そこで、線形回帰を計算した後は

* 誤差が説明変数の値によらず一定か？
* 誤差が正規分布に従っているか？
* 外れ値がないか？

を確認し、偏回帰係数の推定の適切さを見積もったり、外れ値と考えた標本点を除去して偏回帰係数の推定値が適切な値になるかを実際にやってみて検討したりします。これを**回帰診断**（regression diagnostics）といいます。

回帰診断はR言語の場合、`plot` 関数に線形回帰の計算結果を渡すことで実行できます。

```{r}
# 残差分析
par(family = "ヒラギノ角ゴシック W3")
par(mfrow = c(2, 2))
plot(result)
```

**<font color = 59B9C6>問題</font>** : 上の図を参考に、以下の事項について検討してください。

1. 除去するべき外れ値はあるか？
2. 誤差は説明変数によらず一定か？
3. 誤差は正規分布に従っているとみなせるか？

**<font color = 59B9C6>解答</font>** : 
上の図には、`Fitted values`, `Residual`, `Standardized residual`, `Theoretical Quantities`, `Leverage`, `Cook's distance` という6つの単語が出てくるため、これを解説します。

* `Fitted values` : 目的変数の予測値のこと。
* `Residuals` : 目的変数の実測値と予測値の差のこと。
* `Standardized residuals` : 各標本点における誤差 $\epsilon_i$ の$z$得点の推定値のこと。
* `Theoretical Quantities` : `Standardized residuals` を小さい順に並べたとき、対応する標準正規分布の分位点の値のこと。
* `Leverage` : その標本点の目的変数の実現値を変化させたとき、予測値がどれだけ変化するか。数式で表すと $\partial\hat{y_i}/\partial y_i$ のこと。日本語では「てこ比」という。
* `Cook's distance` : 次のように定義される外れ値の評価指標のひとつ。`Standardized Residual` が大きいほど、また `Leverage` が大きいほど外れ値だと考えることができるため、このような式になっている。

$$
\begin{align*}
\text{Cook's distance} &= \text{Standardized Residual} * \frac{\text{Leverage}}{1-\text{Leverage}}
\end{align*}
$$
1. 除去するべき外れ値はあるか？: 4番目の図から、37行目の標本点のCook距離が大きく、外れ値であろうと推測できます。また、1番目の図と3番目の図からも、37行目の標本点だけが大きな誤差の実現値を持っていると推定できる。ゆえに、37行目の標本点が外れ値であろうと考えられる。

2. 誤差は説明変数によらず一定か？ : 1番目の図と3番目の図はいずれも、誤差の推定値と予測値の間の関係を表したものです。いずれも、予測値によらず誤差の値は同じ範囲を散布していることから、誤差は説明変数によらず一定であろうと推測できます。

3. 誤差は正規分布に従っているとみなせるか？ : 2番目の図は誤差の$z$得点の推定値と対応する標準正規分布の推定値を比較したものです。37行目を除いて、直線 $y=x$ に点が並んでいることからから、誤差は正規分布に従っていると仮定して良いだろうことがわかります。

以上で解答は終わりです。■

なお、今回の問題で外れ値と考えた37行目を除去して改めて線形回帰を行うと、次のような結果になります。回帰診断の結果、外れ値は十分に除去できており、また偏回帰係数の推定も適切に行えているだろうことが推測できます。

```{r}
result3 <- lm(formula = 月給 ~ ., 
              data = dat[-37, ])
summary(result3)
```

```{r}
# 残差分析
par(family = "ヒラギノ角ゴシック W3")
par(mfrow = c(2, 2))
plot(result3)
```


# 4. 外挿
早速ですが、以下の問題を考えてみましょう。

**<font color = 59B9C6>問題</font>** :

1. 勤続年数・仕事の達成度・特殊免許の有無の3変数を用い、また37行目を除去したデータを用いて、月給を説明する線形回帰を計算してください。
2. 1の結果を用いて、勤続年数が40年、仕事の達成度が200、特殊免許を持っている社員の月給を予測してください。
3. 2の結果をみた分析者は、一般にこのような高い月給が発生することはなく、予測が妥当ではないと考えました。ではなぜ、このような現実的ではない予測が得られたのか、その理由を考えてください。

**<font color = 59B9C6>解答</font>** : 
線形回帰は以下のように計算できます。

```{r}
result4 <- lm(formula = 月給 ~ 勤務年数+仕事の達成度+特殊免許の有無, 
              data = dat[-37, ])
summary(result4)
```

この結果、勤続年数が40年、仕事の達成度が200、特殊免許を持っている社員の月給は次のように予測できます。

$$
\begin{align*}
199787 + 4958\times40 + 151\times200 + 5310\times1 &= 433617
\end{align*}
$$
さて、分析者はこの予測について「一般にこのような高い月給が発生することはなく、予測が妥当ではない」と考察しています。このような現実的ではない予測が生じた理由は、データに含まれる説明変数の値の範囲を確認するとわかります。

```{r}
# 説明変数の値の範囲
summary(dat)
```

勤務年数は最大で19年の社員しかデータには含まれていません。つまり今回、月給が線形回帰で予測できることを確かめることができたのは、勤務年数が19年までの社員です。実際、勤続年数が長くなるに従って、昇給額は小さくなり線形回帰があてはまらなくなる可能性があります。今回もこのような理由で、勤続年数が40年の社員には線形回帰の予測値が妥当な額にならない現象が発生したと考えられます。■

このように、線形回帰の計算に用いたデータに含まれる説明変数の値の範囲を超えた値をもつような標本点に対して、得られた線形回帰による予測を行うことを**外挿**（extrapolation）といいます。