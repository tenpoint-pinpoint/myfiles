# glmの使い方

x1 <- c(6.5, 3.8, 3.4, 2.4, 3.0, 5.5, 2.4, 6.6)
x2 <- c(3.7, 4.9, 1.0, 1.8, 4.6, 4.8, 3.8, 2.7)
y1 <- c(8, 5, 2, 0, 1, 11, 4, 9)

# glmでは以下の３つの要素を指定する
# 線形予測子　…　formulaに入力、説明変数と目的変数を指定する
# 誤差構造　　…　familyに入力
# リンク関数　…　linkに入力

glm(formula = y1 ~ x1+x2, family = poisson(link="log"))

#--結果-----------------------------------------------#
# Coefficients:
# (Intercept)           x1           x2  
#     -1.3277       0.4039       0.2770  

# Degrees of Freedom: 7 Total (i.e. Null);  5 Residual
# Null Deviance:	    26.78 
# Residual Deviance: 7.085 	AIC: 36.69
#-----------------------------------------------------#

# 同じことをデータフレームにしてやってみる,formulaは省略可能なので省略
df_gp01 <- data.frame(x_1 = x1, x_2 = x2, y_1 = y1)
glm(y_1 ~ x_1+x_2, family = poisson(link="log"), data=df_gp01)

#--結果-----------------------------------------------#
# Coefficients:
# (Intercept)           x1           x2  
#     -1.3277       0.4039       0.2770  

# Degrees of Freedom: 7 Total (i.e. Null);  5 Residual
# Null Deviance:	    26.78 
# Residual Deviance: 7.085 	AIC: 36.69
#-----------------------------------------------------#

# 結果に名前をつけて保存しておく
result01 <- glm(y_1 ~ x_1+x_2, family = poisson(link="log"), data=df_gp01)

# 結果をsummaryに投げ込むと、詳しい結果をみることができる、下記２つは同じ結果を表示する
summary(result01)
summary(glm(y_1 ~ x_1+x_2, family = poisson(link="log"), data=df_gp01))

#--結果-----------------------------------------------#
# Deviance Residuals: 
#        1         2         3         4         5         6         7         8  
# -0.71757   0.09939   0.49348  -1.51703  -1.43278   0.56161   1.24101   0.32697  

#Coefficients:
#  Estimate Std. Error z value Pr(>|z|)    
#  (Intercept)  -1.3277     0.9081  -1.462 0.143716    
#  x_1           0.4039     0.1084   3.726 0.000195 ***
#  x_2           0.2770     0.1561   1.775 0.075949 .  
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# (Dispersion parameter for poisson family taken to be 1)

# Null deviance: 26.777  on 7  degrees of freedom
# Residual deviance:  7.085  on 5  degrees of freedom
# AIC: 36.688
#-----------------------------------------------------#

# glm関数の結果を取り出すための関数が用意されている

formula(result01) # 式を取り出せる
#--結果-----------------------------------------------#
# _1 ~ x_1 + x_2
#-----------------------------------------------------#

coefficients(result01) # 係数を取り出せる
coef(result01)         # どちらも同じ結果
#--結果-----------------------------------------------#
# (Intercept)         x_1         x_2 
#  -1.3277360   0.4039362   0.2770277 
#-----------------------------------------------------#

coef(result01)[["(Intercept)"]] # 係数を１つづつ取り出せる
coef(result01)[["x_1"]]
coef(result01)[["x_2"]]
#--結果-----------------------------------------------#
# [1] -1.327736
# [1] 0.4039362
# [1] 0.2770277
#-----------------------------------------------------#

##################################################################################################################################
##################################################################################################################################

#--モデルの予測値-----------------------------------------------------------------------#
#モデルが予測する目的変数の期待値をモデルの予測値と呼ぶ。リンク関数＝線形予測子であるから
#線形予測子の値がわかれば目的変数の期待値はすぐ計算できる
#---------------------------------------------------------------------------------------#

# 各データ点の線形予測子の値は、係数の値とデータの値から
df_gp01$x_1*coef(result01)[["x_1"]] +
df_gp01$x_2*coef(result01)[["x_2"]] +
coef(result01)[["(Intercept)"]]
#--結果-----------------------------------------------#
# [1] 2.3228516 1.5646572 0.3226747 0.1403607 1.1583999 2.2236459 0.6944161 2.0862176
#-----------------------------------------------------#

# 対応する目的変数の期待値は、この場合対数リンクなので対数の逆関数である指数関数となり
exp(df_gp01$x_1*coef(result01)[["x_1"]] +
    df_gp01$x_2*coef(result01)[["x_2"]] +
    coef(result01)[["(Intercept)"]])
#--結果-----------------------------------------------#
# [1] 10.204733  4.781036  1.380816  1.150689  3.184833  9.240961  2.002539  8.054392
#-----------------------------------------------------#

# ただし、glmには予測値を計算する関数が準備されている。下記２つは同じ結果になる
fitted(result01)
predict(result01, type = "response") 
#--結果-----------------------------------------------#
# 10.204733  4.781036  1.380816  1.150689  3.184833  9.240961  2.002539  8.054392
#-----------------------------------------------------#

# predict関数はtypeをlinkにすることで、各データ点での線形予測子の値が計算できる
predict(result01, type = "link") 
#--結果-----------------------------------------------#
# 2.3228516 1.5646572 0.3226747 0.1403607 1.1583999 2.2236459 0.6944161 2.0862176 
#-----------------------------------------------------#

# x_1が1でx_2が3のときの線形予測子の値、x_1が2でx_2が4のときの線形予測子の値を計算する
# 係数の値を使って以下のようにも計算ができる
1*coef(result01)[["x_1"]] + 3*coef(result01)[["x_2"]] + coef(result01)[["(Intercept)"]]
2*coef(result01)[["x_1"]] + 4*coef(result01)[["x_2"]] + coef(result01)[["(Intercept)"]]
#--結果-----------------------------------------------#
# [1] -0.09271676
# [1] 0.5882471
#-----------------------------------------------------#

# predictを使うと以下のように計算ができる
df_new <- data.frame(x_1 = c(1,2), x_2 = c(3,4))
predict(result01, newdata = df_new, type = "link")
#--結果-----------------------------------------------#
#           1           2 
# -0.09271676  0.58824713 
#-----------------------------------------------------#

##################################################################################################################################
##################################################################################################################################

#--残差---------------------------------------------------------------------------------#
# モデルによる予測値と実際に観測された目的変数の値がどれだけ違うかを残差(residual)と呼ぶ。
# glmの結果に対してresiduals(resid)という関数を使うことにより各種の残差が得られる。
# 残差の種類はtypeで指定する
#---------------------------------------------------------------------------------------#

# 目的変数の実際の観測された値と、モデルの予測値の差は、type = responseにする
resid(result01, type = "response")
#--結果-----------------------------------------------
#          1          2          3          4          5          6          7          8 
# -2.2047332  0.2189643  0.6191839 -1.1506887 -2.1848333  1.7590385  1.9974606  0.9456077 
#-----------------------------------------------------

# 検算してみる
df_gp01$y_1 - predict(result01, type = "response")
#--結果-----------------------------------------------#
#          1          2          3          4          5          6          7          8 
# -2.2047332  0.2189643  0.6191839 -1.1506887 -2.1848333  1.7590385  1.9974606  0.9456077 
#-----------------------------------------------------#


# 目的変数の実際の値は元のデータを見てもわかる。結果オブジェクトのdataという項目である
result01$data
#--結果-----------------------------------------------#
# x_1 x_2 y_1
# 1 6.5 3.7   8
# 2 3.8 4.9   5
# 3 3.4 1.0   2
# 4 2.4 1.8   0
# 5 3.0 4.6   1
# 6 5.5 4.8  11
# 7 2.4 3.8   4
# 8 6.6 2.7   9
#-----------------------------------------------------#

# 上記を利用し、残差を計算することも可能
result01$data$y_1 - predict(result01, type = "response")
#--結果-----------------------------------------------#
#          1          2          3          4          5          6          7          8 
# -2.2047332  0.2189643  0.6191839 -1.1506887 -2.1848333  1.7590385  1.9974606  0.9456077 
#-----------------------------------------------------#

##################################################################################################################################
##################################################################################################################################

#--デビアンス残差-----------------------------------------------------------------------#
# デビアンス（逸脱度）とは対数尤度比統計量のこと
# デビアンスとは同様な２つのモデルの対数尤度の差の２倍（計算方法は各データ点の差をとって、２倍して、平方根をとったもの）
# 言い換えると、「データが到達可能な最大対数尤度」と「回帰パラメータの最尤推定値における対数尤度」の差に2を掛けたもの
# 「データが到達可能な最大対数尤度」とは、完璧な予測ができた場合の対数尤度
# 「回帰パラメータの最尤推定値における対数尤度」とは
# デビアンス残差は各データ点ごとのデビアンスの平方根に（目的変数の実測値-予測値）の符号を掛けたもの
# residualsではtypeをdevianceと指定する
#---------------------------------------------------------------------------------------#

residuals(result01, type = "deviance")
resid(result01, type = "deviance")
#--結果-----------------------------------------------#
#           1           2           3           4           5           6           7           8 
# -0.71756855  0.09939095  0.49347962 -1.51702916 -1.43278283  0.56161229  1.24100973  0.32697208 
#-----------------------------------------------------#

#上記を確認してみる。上から（目的変数-予測値）の符号、デビアンスの平方根、２つの積
a01 <- sign(df_gp01$y_1 - predict(result01, type = "response"))
b01 <- sqrt(2 * (log(dpois(df_gp01$y_1, df_gp01$y_1))-log(dpois(df_gp01$y_1, predict(result01, type = "response")))))
a01 * b01
#--結果-----------------------------------------------#
#  1  2  3  4  5  6  7  8 
# -1  1  1 -1 -1  1  1  1 
?dpois
#[1] 0.71756855 0.09939095 0.49347962 1.51702916 1.43278283 0.56161229 1.24100973 0.32697208

#           1           2           3           4           5           6           7           8 
# -0.71756855  0.09939095  0.49347962 -1.51702916 -1.43278283  0.56161229  1.24100973  0.32697208 
#-----------------------------------------------------#

# また、residualsはtypeを省略するとデビアンス残差になる
residuals(result01)
#--結果-----------------------------------------------#
#           1           2           3           4           5           6           7           8 
# -0.71756855  0.09939095  0.49347962 -1.51702916 -1.43278283  0.56161229  1.24100973  0.32697208 
#-----------------------------------------------------#

##################################################################################################################################
##################################################################################################################################

#--ピアソン残差--------------------------------------------------------------------------#
#ピアソン残差とは、生の残差を平方根平均二乗誤差で除算した値
#（目的変数の実測値-予測値）を分散の平方根で割ったもの
#平均二乗誤差とは、真のモデルと予測モデルとの誤差。計算方法は標準偏差とほぼ同じ（標本平均が真の値になる）
#観測値がどの程度モデルで予測できるかの指標。モデルの適合度が低い時、大きくなる
#residualsではtypeをpearsonと指定する
#---------------------------------------------------------------------------------------#

residuals(result01, type = "pearson")
#--結果-----------------------------------------------#
#          1          2          3          4          5          6          7          8 
# -0.6901686  0.1001411  0.5269286 -1.0727016 -1.2242636  0.5786511  1.4115221  0.3331920 
#-----------------------------------------------------#

#確認してみる ※ポアソン分布なので平均＝分散
(df_gp01$y_1 - predict(result01, type = "response")) / sqrt(predict(result01, type = "response"))
#--結果-----------------------------------------------#
#          1          2          3          4          5          6          7          8 
# -0.6901686  0.1001411  0.5269286 -1.0727016 -1.2242636  0.5786511  1.4115221  0.3331920 
#-----------------------------------------------------#

##################################################################################################################################
##################################################################################################################################

#--デフォルトのリンク-------------------------------------------------------------------#
# Rのglm関数では誤差構造が決まると、linkを設定していない場合には、誤差構造ごとのデフォルト
# として決められているリンクが自動的に使われる。
# ex.誤差構造：ポアソン分布　＝　link：対数リンク
#---------------------------------------------------------------------------------------#

# 以下の２つは同じ結果になる
glm(y_1~x_1+x_2, family = poisson, data = df_gp01)
glm(y_1~x_1+x_2, family = poisson(link = "log"), data = df_gp01)
#--結果-----------------------------------------------#
# Call:  glm(formula = y_1 ~ x_1 + x_2, family = poisson, data = df_gp01)
# Coefficients:
#   (Intercept)          x_1          x_2  
#       -1.3277       0.4039       0.2770  

# Degrees of Freedom: 7 Total (i.e. Null);  5 Residual
# Null Deviance:	    26.78 
# Residual Deviance: 7.085 	AIC: 36.69
#-----------------------------------------------------#

##################################################################################################################################
##################################################################################################################################


#--検定---------------------------------------------------------------------------------#
# summaryでは検定結果が表示される
# 以下で示されている検定結果はwald検定の結果である
summary(result01)
#--結果-----------------------------------------------#
# Coefficients:
#            Estimate Std.      Error      z    value Pr(>|z|)    
# (Intercept)     -1.3277     0.9081  -1.462  0.143716    
# x_1              0.4039     0.1084   3.726  0.000195 ***
# x_2              0.2770     0.1561   1.775  0.075949 .  

# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#-----------------------------------------------------#

##################################################################################################################################
##################################################################################################################################

#--尤度比検定---------------------------------------------------------------------------#
# 尤度比検定を行う場合はanovaを使い、glmの結果が入っているオブジェクトと検定統計量を指定する
# 尤度比検定の場合は、対数尤度の差の二倍（デビアンスや対数尤度比統計量）がカイ二乗分布する
# 性質を使うため、検定統計量はカイ二乗である（擬似尤度の場合はF）
# 新しいRでは test = "Chisq" の代わりにtest = "LRT" でも尤度比検定が行える
#---------------------------------------------------------------------------------------#

anova(result01, test = "LRT")
# x_1の部分に示されているのは定数項だけのモデルと、説明変数がx_1及び定数項のモデルを比較した結果である
# x_2の部分に示されているのは定数項+x_1のモデルと、定数項+x_2のモデルを比較した結果である
#--結果-----------------------------------------------#
# Analysis of Deviance Table
# Model: poisson, link: log

# Response: y_1
# Terms added sequentially (first to last)

#       Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
# NULL                     7     26.777              
# x_1   1  16.2114         6     10.566 5.665e-05 ***
# x_2   1   3.4807         5      7.085   0.06209 .  

#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#-----------------------------------------------------#


# anovaでは２つのモデルをそれぞれ指定して比較することもできる。定数項だけのモデルと説明変数がx_1及び定数項のモデルを比べると
resgp01_0 <- glm(y_1 ~ 1, family = poisson(link = "log"), data = df_gp01)
resgp01_1 <- glm(y_1 ~ x_1, family = poisson(link = "log"), data = df_gp01)
resgp01_2 <- glm(y_1 ~ x_2, family = poisson(link = "log"), data = df_gp01)

anova(resgp01_0,resgp01_1, test = "LRT")

#--結果-----------------------------------------------#
# Analysis of Deviance Table

# Model 1: y_1 ~ 1
# Model 2: y_1 ~ x_1
# Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    
# 1      7     26.777                                …　定数項のみ
# 2      6     10.566  1   16.211 5.665e-05 ***      …　定数項のみと、説明変数がx_1と定数項のモデルを比較

#  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#-----------------------------------------------------#


# 次に説明変数がx_1 + 定数項のモデルと、説明変数がx_1 + x_2 + 定数項のモデルを比較する
anova(resgp01_1,result01, test = "LRT")

#--結果-----------------------------------------------#
# Analysis of Deviance Table

# Model 1: y_1 ~ x_1
# Model 2: y_1 ~ x_1 + x_2
# Resid. Df Resid. Dev Df Deviance Pr(>Chi)  
# 1      6     10.566                       
# 2      5      7.085  1   3.4807  0.06209 .

# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#-----------------------------------------------------#


# 尤度比検定は２つのモデルを比べる。anovaでは複数の説明変数があるときに、まず定数項だけというモデルをまず考え、
# そこに説明変数を１つずつ加えたモデルを考えて、ある１つの説明変数を加える前後のモデルを比べる。
# 説明変数を加える順序は、formulaで書いた順序である。
# よって、尤度比検定は可能だが比較されないモデルの組み合わせがある。上の例では定数項だけのモデルと、定数項+x_2のモデルは比較されていない。
# 説明変数の順序を変える、もしくは個別にモデルを挿入して比較する必要がある

# 尤度比検定に関数関数としてdrop1とadd1もある。
# drop1という関数は、anovaとは違い、指定したモデルとそこから１つ説明変数を除いたモデルを比べる
drop1(result01, test = "Chisq")

#--結果-----------------------------------------------#
# Single term deletions

# Model:
#   y_1 ~ x_1 + x_2
#         Df Deviance    AIC     LRT  Pr(>Chi)    
# <none>       7.085  36.688                      
# x_1     1   22.802  50.405 15.7171 7.355e-05 ***　　　…　x_1とx_2を説明変数とするモデルと、x_2を説明変数とするモデルの比較
# x_2     1   10.566  38.168  3.4807   0.06209 .  　　　…　x_1とx_2を説明変数とするモデルと、x_1を説明変数とするモデルの比較

# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#-----------------------------------------------------#

# add1は、dropの反対で、指定したモデルとそこに１つ説明変数を加えたモデルを比べる
add1(resgp01_0, ~x_1, test = "Chisq")

#--結果-----------------------------------------------#
# Single term additions

# Model:
#  y_1 ~ 1
#         Df Deviance    AIC    LRT  Pr(>Chi)    
# <none>      26.777  52.380                     
# x_1     1   10.566  38.168 16.211 5.665e-05 ***      …　定数項だけのモデルと、定数項とx_1を説明変数とするモデルの比較

# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#-----------------------------------------------------#

add1(resgp01_0, ~x_1 + x_2, test = "Chisq")

#--結果-----------------------------------------------#
# Single term additions

# Model:
#  y_1 ~ 1
#         Df  Deviance     AIC     LRT  Pr(>Chi)   
#<none>         26.777  52.380                     
# x_1     1     10.566  38.168  16.211 5.665e-05 ***   …　定数項だけのモデルと、定数項とx_1を説明変数とするモデルの比較
# x_2     1     22.802  50.405   3.975   0.04618 *     …　定数項だけのモデルと、定数項とx_2を説明変数とするモデルの比較

# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#-----------------------------------------------------#


# summaryで表示されるWald検定は、他の説明変数が全て使われている時に、その説明変数の効果を見ている
# よって尤度比検定でいえば、全ての説明変数が使われているモデルとその説明変数だけを除いたモデルの比較に対応する

resgp01_0 <- glm(y_1 ~ 1, family = poisson(link = "log"), data = df_gp01)
resgp01_1 <- glm(y_1 ~ x_1, family = poisson(link = "log"), data = df_gp01)
resgp01_2 <- glm(y_1 ~ x_2, family = poisson(link = "log"), data = df_gp01)
summary(result01)
#--結果-----------------------------------------------#
# Coefficients:
#            Estimate Std.      Error      z    value Pr(>|z|)    
# (Intercept)     -1.3277     0.9081  -1.462  0.143716    
# x_1              0.4039     0.1084   3.726  0.000195 ***　　　…　anova(resgp01_2, result01, test = "Chisq")　に対応
# x_2              0.2770     0.1561   1.775  0.075949 .  　　　…　anova(resgp01_1, result01, test = "Chisq")　に対応

# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#-----------------------------------------------------#

# なお、Wald検定はパッケージcarの関数Anovaで、test.statisticを"Wald"と指定することでも実行可能である
# ただし検定に使われる量（統計検定量）はカイ二乗で、glmでのsummaryにおけるzを二乗したものである
# また、glmでのスコア検定はパッケージstatmodの関数glm.scoretestで実行可能である
# anova（glmのオブジェクト、test="Rao"）としてスコア検定を行える

##################################################################################################################################
##################################################################################################################################


# 対数尤度とデビアンス

# 対数尤度は以下で計算可能
logLik(result01)
#--結果-----------------------------------------------#
# 'log Lik.' -15.34385 (df=3)
#-----------------------------------------------------#

# デビアンス残差（summaryのResidual deviance）は以下で計算可能
deviance(result01)
#--結果-----------------------------------------------#
# [1] 7.084974
#-----------------------------------------------------#

summary(result01)
#--結果-----------------------------------------------#
# Null deviance      : 26.777  on 7  degrees of freedom
# Residual deviance  :  7.085  on 5  degrees of freedom
#-----------------------------------------------------#

# Null deviance　　　…　説明変数が含まれず定数項（切片）だけのモデルと飽和モデルの対数尤度の差の２倍
# Residual deviance　…　説明変数を含むモデルと飽和モデルの対数尤度の差の２倍
# 普通デビアンスと呼ばれるのはResidual devianceである
# Null devianceとResidual devianceの差は、x_1とx_2と切片を含むモデルと切片だけのモデルの対数尤度の差の２倍である
# 切片だけのモデルの対数尤度は

logLik(resgp01_0)
#--結果-----------------------------------------------#
# 'log Lik.' -25.18991 (df=1)
#-----------------------------------------------------#

# 対数尤度の差は
2*(logLik(result01) - logLik(resgp01_0))
#--結果-----------------------------------------------#
# 'log Lik.' 19.69212 (df=1)
#-----------------------------------------------------#

# となり、Null devianceとResidual devianceの差に等しい。
# ここでの飽和モデルとは、モデルの予測値と目的変数の実際の値が等しい場合である。この飽和モデルの対数尤度を計算すると

sum(log(dpois(df_gp01$y_1, lambda = df_gp01$y_1)))    # 飽和モデル、期待値が全て実際の値と同じ
#--結果-----------------------------------------------#
# [1] -11.80137
#-----------------------------------------------------#

# Null devianceとResidual devianceを計算してみる

# Null deviance
2 * (sum(log(dpois(df_gp01$y_1, lambda = df_gp01$y_1))) - logLik(resgp01_0))
#--結果-----------------------------------------------#
# 'log Lik.' 26.77709 (df=1)
#-----------------------------------------------------#

# Residual deviance
2 * (sum(log(dpois(df_gp01$y_1, lambda = df_gp01$y_1))) - logLik(result01))
#--結果-----------------------------------------------#
# 'log Lik.' 7.084974 (df=3)
#-----------------------------------------------------#

##################################################################################################################################
##################################################################################################################################

# AIC  …　AICの計算方法は　[-2 * 対数尤度 + 2 * パラメータ数]

# AICは以下で計算可能
AIC(resgp01_2)
#--結果-----------------------------------------------#
# [1] 50.40485
#-----------------------------------------------------#

# こちらはパラメータ数もわかる  最初の数値がパラメータの数、２つめの数がAIC
extractAIC(result01)
#--結果-----------------------------------------------#
# [1] 3.00000 36.68771
#-----------------------------------------------------#

# 検定の項目で述べたdrop1とadd1を繰り返して、１つの説明変数を含むモデルと含まないモデルで、デビアンスやAICを次々に比較する関数として
# step関数、stepAIC関数がある

##################################################################################################################################
##################################################################################################################################

# 線形予測子とその指定

# glmでは、formulaが y~x_1 なら、目的変数yの期待値＝β・x + α　という式で、係数βとαが推定されることを意味する
# formulaの部分ではマイナスの記号は一般に「その項がない」ことを意味する。切片がないことは -1 で指定する
# y~x_1 なら、定数項がない、目的変数yの期待値β・xという式で、係数βだけが推定される

# 定数項αがない場合
glm(y_1 ~ x_1 -1, family = poisson(link = "log"), data = df_gp01)
glm(y_1 ~ x_1 +0, family = poisson(link = "log"), data = df_gp01) # 定数項がないという表現が違うだけ
#--結果-----------------------------------------------#
# Call:  glm(formula = y_1 ~ x_1 - 1, family = poisson(link = "log"), data = df_gp01)

# Coefficients:
#  x_1  
#  0.348  

# Degrees of Freedom: 8 Total (i.e. Null);  7 Residual
# Null Deviance     :	91.53 
# Residual Deviance : 10.75 	AIC: 36.35
#-----------------------------------------------------#

# 定数項だけの場合
glm(y_1 ~ 1, family = poisson(link = "log"), data = df_gp01)
#--結果-----------------------------------------------#
# Call:  glm(formula = y_1 ~ 1, family = poisson(link = "log"), data = df_gp01)

# Coefficients:
#  (Intercept)  
# 1.609  

# Degrees of Freedom: 7 Total (i.e. Null);  7 Residual
# Null Deviance     :	26.78 
# Residual Deviance : 26.78 	AIC: 52.38
#-----------------------------------------------------#

# 指定したデータフレーム内の変数を全て指定する場合は、「.」を使う
glm(y_1 ~ ., family = poisson(link = "log"), data = df_gp01)
glm(y_1 ~ x_1 + x_2 , family = poisson(link = "log"), data = df_gp01) # 上の式はこれと同じ意味
#--結果-----------------------------------------------#
# Call:  glm(formula = y_1 ~ ., family = poisson(link = "log"), data = df_gp01)

# Coefficients:
#  (Intercept)          x_1          x_2  
#      -1.3277       0.4039       0.2770  

# Degrees of Freedom: 7 Total (i.e. Null);  5 Residual
# Null Deviance     : 26.78 
# Residual Deviance : 7.085 	AIC: 36.69
#-----------------------------------------------------#

# あるモデルに他の説明変数を追加した（あるいは減らした）モデルは、改めて新しい説明変数をformulaに書いたglmを使わなくても
# 説明変数の違いの部分だけを指定して、updateでも行うことができる

update(resgp01_0, . ~ . + x_1 + x_2)  #定数項だけのモデルに、x_1とx_2を追加して再計算
glm(y_1 ~ ., family = poisson(link = "log"), data = df_gp01) #これと一致する
#--結果-----------------------------------------------#
#Call:  glm(formula = y_1 ~ x_1 + x_2, family = poisson(link = "log"), data = df_gp01)

# Coefficients:
#  (Intercept)          x_1          x_2  
#      -1.3277       0.4039       0.2770  

# Degrees of Freedom: 7 Total (i.e. Null);  5 Residual
# Null Deviance     :	26.78 
# Residual Deviance : 7.085 	AIC: 36.69
#-----------------------------------------------------#

# updateではピリオドの意味がglmとは異なっている、全ての変数ではなく、指定したモデルで使われている変数、という意味になる。
# .~. はglmなどでの使い方から連想されるようなデータフレーム中全ての説明変数を使用、という意味ではなく指定したモデルと同じ、という意味になる

##################################################################################################################################
##################################################################################################################################


#線形予測子の指定　：　交互作用

# ある説明変数が１増えたときに、線形予測子すなわちリンク関数（目的関数の期待値）に与える影響は、その説明変数の係数である
# 今使っている例のような対数リンクであればlog（期待値）に与える影響がその説明変数の係数であるということになる
# しかし現実にはある説明変数が１増えたときの効果は、他の説明変数がどんな値なのかによることもある。そのように他の説明変数に
#　依存している場合を扱うために交互作用を用いる
# glmでは交互作用は「:」という記号で表す。説明変数x＿１とx＿２の間の交互作用ならx_1:x_2である
# また「*」を使うことによりここの説明変数と交互作用の式を作ることができる

glm(y_1 ~ x_1 + x_2 + x_1:x_2, family = poisson(link = "log"), data = df_gp01)
glm(y_1 ~ x_1　* x_2, family = poisson(link = "log"), data = df_gp01)                       # 上記式と同じ結果
glm(formula = y_1 ~ (x_1+x_2)*(x_1+x_2), family = poisson(link = "log"), data = df_gp01 )   # 上記式と同じ結果
glm(formula = y_1 ~ (x_1+x_2)^2, family = poisson(link = "log"), data = df_gp01 )           # 上記式と同じ結果
#--結果-----------------------------------------------#
# Call:  glm(formula = y_1 ~ x_1 + x_2 + x_1:x_2, family = poisson(link = "log"), data = df_gp01)

# Coefficients:
#  (Intercept)          x_1          x_2      x_1:x_2  
#    -1.415906     0.423864     0.302924    -0.005829  

# Degrees of Freedom : 7 Total (i.e. Null);  4 Residual
# Null Deviance      :	    26.78 
# Residual Deviance  : 7.082 	AIC: 38.69
#-----------------------------------------------------#


#線形予測子の指定　：　計算式そのまま

# formulaの部分では*や＋やーなどの記号が、これまで述べてきたように特別な意味を持つ。しかし、変数の積や和などを扱いたいこともある。
# その時は、I()で囲んだ中に入れれば、その計算式そのものを意味する

glm(y_1 ~ x_1 + I(x_2*x_2+x_2), family = poisson(link = "log"), data = df_gp01)
#--結果-----------------------------------------------#
# call:  glm(formula = y_1 ~ x_1 + I(x_2 * x_2 + x_2), family = poisson(link = "log"), data = df_gp01)

# Coefficients:
#   (Intercept)                x_1  I(x_2 * x_2 + x_2)  ←　計算式がそのまま変数になっている
#     -1.06992             0.42144             0.03634  

# Degrees of Freedom: 7 Total (i.e. Null);  5 Residual
# Null Deviance:	    26.78 
# Residual Deviance: 6.996 	AIC: 36.6
#-----------------------------------------------------#


# I()は説明変数の２次以上の項などにも使える

glm(y_1 ~ x_1 + I(x_1^2), family = poisson(link = "log"), data = df_gp01)
#--結果-----------------------------------------------#
# Call:  glm(formula = y_1 ~ x_1 + I(x_1^2), family = poisson(link = "log"), data = df_gp01)

# Coefficients:
#  (Intercept)          x_1     I(x_1^2)  
#      -3.4378       1.9166      -0.1613  

# Degrees of Freedom: 7 Total (i.e. Null);  5 Residual
# Null Deviance:	    26.78 
# Residual Deviance: 8.423 	AIC: 38.03
#-----------------------------------------------------#
